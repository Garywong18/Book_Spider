# Book_Spider
爬取网络上主要的图书平台
### 主要爬取思路
- 通过首页的url获取大分类，中分类，小分类的名字以及图书列表页的url
- 发送图书列表页的url
- 通过图书列表解析函数解析每一本图书目标字段
- 如果遇到某个字段是通过异步加载的，找到加载的url，发送请求，新增解析函数得到字段
- 翻页，回调图书列表解析函数
### 遇到的一些问题
- 关于在传递item时候为什么要使用`deepcopy()`
  - 因为所有大分类下面的中分类和小分类是共用同一个item，当item在parse函数中获得大分类，中分类，小分类等字段后将item传递给了parse_list函数，由于scrapy是多线程异步框架，所以在parse_list函数爬取图书详情字段的同时，parse函数会继续爬取新的小分类，这时候会将之前爬取到的小分类覆盖掉。所以为了使已经传递给parse_list的item不受影响，应该将item进行deepcopy，即在内存中开辟一块新的位置，而不是引用之前item的地址。
  - 而之前写的只有一个分类的爬虫，之所以不需要deepcopy是因为parse函数将item传递给parse_list后会新创建一个item。
- 在翻页的时候切莫忘记传递item
  - 因为翻页后回调的是parse_list，如果没有传递item的话之前在parse函数爬取的大分类，中分类，小分类等字段将会丢失，并且由于parse_list函数开头有接收item的代码`item = response.meta['item']`如果接收不到item的话会报错误`KeyError:Item`
- 在字符串拼接的时候，不要忘记先判断字符串时候为None
- 京东图书的价格是通过异步加载的json数据
  - 先通过搜索价格找到发送价格的url
  - 分析url的特点，即找到url和每一本书的连接点
  - 重新构造url
  - 发送请求，获取包含价格的json相应
  - 通过价格解析函数解析json得到价格
  - 注意如果新url的域名不在待爬范围要将域名添加到allowed_domains中去
